# Efficient-LLMAgent-Survey
This repository maintains a curated list of papers related to Large Language Model Based Agents (LLM Agents), especially focusing on efficient serving methods for LLM Agents.

This paper list covers several main aspects of efficient serving methods for LLM Agents. 
Table of content:

- [Efficient-LLMAgent-Survey](#efficient-llmagent-survey)
  - [What is LLM Agent](#what-is-llm-agent)
  - [Efficient Serving LLM Agent](#efficient-serving-llm-agent)
    - [LLM Serving](#llm-serving)
    - [Planning](#planning)
    - [Tool and Action](#tool-and-action)
      - [Serverless](#serverless)
    - [Memory](#memory)
    - [Agent Framework](#agent-framework)
  - [Component Collaboration](#component-collaboration)
  - [Device-Edge-Cloud Collaboration](#device-edge-cloud-collaboration)
  - [LLM and LLM Agent Framework](#llm-and-llm-agent-framework)
  - [Benchmark, Trace, and Dataset](#benchmark-trace-and-dataset)
  - [LLM and Agent on Mobile Platform](#llm-and-agent-on-mobile-platform)
    - [LLM for Mobile Platform](#llm-for-mobile-platform)
    - [Agent for Mobile Platform](#agent-for-mobile-platform)
  - [Survey Papers](#survey-papers)
  - [Others](#others)



## What is LLM Agent
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)

## Efficient Serving LLM Agent

### LLM Serving
- [HeteGen](https://arxiv.org/abs/2403.01164) HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices | MLSys'24

- [POLCA](https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GPU_Power_ASPLOS_24.pdf): Characterizing Power Management Opportunities for LLMs in the Cloud | ASPLOS'24

### Planning


### Tool and Action


#### Serverless
- [Serverless LLM](https://arxiv.org/abs/2401.14351): ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models | OSDI'24
  
### Memory

### Agent Framework

## Component Collaboration

Multi-Agent

## Device-Edge-Cloud Collaboration
- [Hybrid LLM](https://openreview.net/forum?id=02f3mUtqnM): Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing | ICLR'24

- [](https://arxiv.org/abs/2306.02003): On Optimal Caching and Model Multiplexing for Large Model Inference | NeurIPS'23
## LLM and LLM Agent Framework


## Benchmark, Trace, and Dataset
- [BurstGPT](https://arxiv.org/abs/2401.17644):Towards Efficient and Reliable LLM Serving: A Real-World Workload Study

## LLM and Agent on Mobile Platform

### LLM for Mobile Platform


### Agent for Mobile Platform


## Survey Papers
- [A Survey on Effective Invocation Methods of Massive LLM Services](https://arxiv.org/abs/2402.03408)

## Others

