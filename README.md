# Efficient-LLMAgent-Survey
This repository maintains a curated list of papers related to Large Language Model Based Agents (LLM Agents), especially focusing on efficient serving methods for LLM Agents.

This paper list covers several main aspects of efficient serving methods for LLM Agents. 
Table of content:

- [Efficient-LLMAgent-Survey](#efficient-llmagent-survey)
  - [What is LLM Agent](#what-is-llm-agent)
  - [Efficient Serving LLM Agent](#efficient-serving-llm-agent)
    - [LLM Serving](#llm-serving)
    - [Planning](#planning)
    - [Tool and Action](#tool-and-action)
      - [Serverless](#serverless)
    - [Memory](#memory)
    - [Agent Framework](#agent-framework)
  - [Component Collaboration](#component-collaboration)
  - [Device-Edge-Cloud Collaboration](#device-edge-cloud-collaboration)
  - [LLM and LLM Agent Framework](#llm-and-llm-agent-framework)
  - [Benchmark, Trace, and Dataset](#benchmark-trace-and-dataset)
  - [LLM and Agent on Mobile Platform](#llm-and-agent-on-mobile-platform)
  - [Survey Papers](#survey-papers)
  - [Others](#others)



## What is LLM Agent
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)

## Efficient Serving LLM Agent

### LLM Serving
- [HeteGen](https://arxiv.org/abs/2403.01164) HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices | MLSys'24

- [POLCA](https://www.microsoft.com/en-us/research/uploads/prodnew/2024/03/GPU_Power_ASPLOS_24.pdf)Characterizing Power Management Opportunities for LLMs in the Cloud | ASPLOS'24

### Planning
- [PreAct](https://arxiv.org/abs/2402.11534)PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability
- [An LLM Compiler for Parallel Function Calling](https://arxiv.org/abs/2312.04511)
- [Dynamic Planning with a LLM](https://arxiv.org/abs/2308.06391)
### Tool and Action
- [ToolChain^*](https://arxiv.org/abs/2310.13227)ToolChain^*: Efficient Action Space Navigation in Large Language Models with A* Search | ICLR'24
- [Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments](https://arxiv.org/abs/2402.14672)
- [Efficient Tool Use with Chain-of-Abstraction Reasoning] (https://arxiv.org/abs/2401.17464)
- [ToolNet](https://arxiv.org/abs/2403.00839) ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph
- [Budget-Constrained Tool Learning with Planning](https://arxiv.org/abs/2402.15960)
#### Serverless
- [Serverless LLM](https://arxiv.org/abs/2401.14351) ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models | OSDI'24
  
### Memory

- [RET-LLM](https://arxiv.org/abs/2305.14322)RET-LLM: Towards a General Read-Write Memory for Large Language Models
- [Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759) | ACL'23
- [Memory Sandbox](https://arxiv.org/abs/2308.01542)Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents

### Agent Framework

## Component Collaboration
focus on improving the efficiency of data exchange and data transmission within AI agents:
Multi-Agent

## Device-Edge-Cloud Collaboration
- [Hybrid LLM](https://openreview.net/forum?id=02f3mUtqnM) Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing | ICLR'24

- [Caching](https://arxiv.org/abs/2306.02003) On Optimal Caching and Model Multiplexing for Large Model Inference | NeurIPS'23

- [Octopus v2](https://arxiv.org/abs/2404.01744v5)Octopus v2: On-device language model for super agent | Stanford 
- 
## LLM and LLM Agent Framework


## Benchmark, Trace, and Dataset
- [BurstGPT](https://arxiv.org/abs/2401.17644)Towards Efficient and Reliable LLM Serving: A Real-World Workload Study

## LLM and Agent on Mobile Platform

- [Mobile LLM](https://arxiv.org/abs/2402.14905) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases | Meta

-[BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models](https://arxiv.org/abs/2402.08219)

## Survey Papers
- [A Survey on Effective Invocation Methods of Massive LLM Services](https://arxiv.org/abs/2402.03408)

## Others

